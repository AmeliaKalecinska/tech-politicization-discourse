{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d23d9b-2e56-423b-aa5e-217fea4a64c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bertopic sentence-transformers scikit-learn matplotlib pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff5351-7e53-4553-90d5-fc314ea6e637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "def check_and_install_packages():\n",
    "    \"\"\"Check if required packages are installed and provide installation instructions\"\"\"\n",
    "    required_packages = {\n",
    "        'bertopic': 'bertopic',\n",
    "        'sentence_transformers': 'sentence-transformers',\n",
    "        'sklearn': 'scikit-learn',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'pandas': 'pandas',\n",
    "        'openpyxl': 'openpyxl'\n",
    "    }\n",
    "    \n",
    "    missing_packages = []\n",
    "    \n",
    "    for package, pip_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            missing_packages.append(pip_name)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(\"Missing required packages. Please install them using:\")\n",
    "        print(f\"pip install {' '.join(missing_packages)}\")\n",
    "        print(\"\\nOr with conda:\")\n",
    "        print(f\"conda install -c conda-forge {' '.join(missing_packages)}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "if not check_and_install_packages():\n",
    "    sys.exit(\"Please install the required packages and restart the kernel.\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NPMIOptimizer:\n",
    "    def __init__(self, data_path, text_columns=['title'], selftext_column='selftext', min_cluster_size=30):\n",
    "        \"\"\"\n",
    "        Initialize the NPMI optimizer for BERTopic with larger minimum cluster size\n",
    "        \n",
    "        Parameters:\n",
    "        data_path: Path to the Excel file\n",
    "        text_columns: List of columns to use for clustering (default: ['title'])\n",
    "        selftext_column: Optional selftext column name\n",
    "        min_cluster_size: Minimum size for clusters (default: 30)\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.text_columns = text_columns\n",
    "        self.selftext_column = selftext_column\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "        self.df = None\n",
    "        self.documents = None\n",
    "        self.embeddings = None\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"Load data and create combined text for clustering\"\"\"\n",
    "        print(\"Loading data...\")\n",
    "        self.df = pd.read_excel(self.data_path)\n",
    "        print(f\"Loaded {len(self.df)} documents\")\n",
    "        \n",
    "        combined_texts = []\n",
    "        for idx, row in self.df.iterrows():\n",
    "            text_parts = []\n",
    "            \n",
    "            for col in self.text_columns:\n",
    "                if col in self.df.columns and pd.notna(row[col]):\n",
    "                    text_parts.append(str(row[col]))\n",
    "            \n",
    "            if (self.selftext_column in self.df.columns and \n",
    "                pd.notna(row[self.selftext_column]) and \n",
    "                str(row[self.selftext_column]).strip() != ''):\n",
    "                text_parts.append(str(row[self.selftext_column]))\n",
    "            \n",
    "            combined_text = ' '.join(text_parts)\n",
    "            combined_texts.append(combined_text if combined_text.strip() else 'empty')\n",
    "        \n",
    "        self.documents = combined_texts\n",
    "        print(\"Text preprocessing completed\")\n",
    "        \n",
    "        print(\"Generating embeddings...\")\n",
    "        self.embeddings = self.embedding_model.encode(self.documents, show_progress_bar=True)\n",
    "        print(\"Embeddings generated\")\n",
    "        \n",
    "    def reassign_outliers(self, df_temp, embeddings, topic_col='topic'):\n",
    "        \"\"\"Reassign outlier documents (-1 topic) to nearest clusters\"\"\"\n",
    "        outlier_indices = np.where(df_temp[topic_col] == -1)[0]\n",
    "        \n",
    "        if len(outlier_indices) == 0:\n",
    "            return df_temp.copy()\n",
    "        \n",
    "        outlier_embeddings = np.array(embeddings)[outlier_indices]\n",
    "\n",
    "        topic_ids = sorted(set(df_temp[topic_col]) - {-1})\n",
    "        \n",
    "        if len(topic_ids) == 0:\n",
    "            return df_temp.copy()\n",
    "        \n",
    "        topic_centroids = []\n",
    "        for topic in topic_ids:\n",
    "            idxs = df_temp[df_temp[topic_col] == topic].index\n",
    "            topic_embeds = np.array(embeddings)[idxs]\n",
    "            centroid = topic_embeds.mean(axis=0)\n",
    "            topic_centroids.append(centroid)\n",
    "        \n",
    "        topic_centroids = np.vstack(topic_centroids)\n",
    "        \n",
    "        sims = cosine_similarity(outlier_embeddings, topic_centroids)\n",
    "        \n",
    "        reassigned_topics = [topic_ids[i] for i in sims.argmax(axis=1)]\n",
    "        \n",
    "        df_result = df_temp.copy()\n",
    "        df_result.loc[outlier_indices, topic_col] = reassigned_topics\n",
    "        \n",
    "        return df_result\n",
    "    \n",
    "    def calculate_npmi(self, documents, topics, top_words_per_topic=10):\n",
    "        \"\"\"\n",
    "        Calculate NPMI (Normalized Pointwise Mutual Information) for topic coherence\n",
    "        \n",
    "        Parameters:\n",
    "        documents: List of documents\n",
    "        topics: List of topic assignments\n",
    "        top_words_per_topic: Number of top words to consider per topic\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(max_features=5000, stop_words='english', \n",
    "                                   min_df=2, max_df=0.95, ngram_range=(1,2))\n",
    "        doc_word_matrix = vectorizer.fit_transform(documents)\n",
    "        vocab = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        topic_docs = defaultdict(list)\n",
    "        for doc_idx, topic in enumerate(topics):\n",
    "            if topic != -1:\n",
    "                topic_docs[topic].append(doc_idx)\n",
    "        \n",
    "        if len(topic_docs) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        topic_npmis = []\n",
    "        \n",
    "        for topic_id, doc_indices in topic_docs.items():\n",
    "            if len(doc_indices) < 2:\n",
    "                continue\n",
    "                \n",
    "            topic_matrix = doc_word_matrix[doc_indices]\n",
    "            word_freqs = np.array(topic_matrix.sum(axis=0)).flatten()\n",
    "            \n",
    "            top_word_indices = word_freqs.argsort()[-top_words_per_topic:][::-1]\n",
    "            top_words = [(idx, word_freqs[idx]) for idx in top_word_indices if word_freqs[idx] > 0]\n",
    "            \n",
    "            if len(top_words) < 2:\n",
    "                continue\n",
    "            \n",
    "            npmi_scores = []\n",
    "            total_docs = len(documents)\n",
    "            \n",
    "            for i, (word1_idx, freq1) in enumerate(top_words):\n",
    "                for j, (word2_idx, freq2) in enumerate(top_words):\n",
    "                    if i >= j:\n",
    "                        continue\n",
    "                    \n",
    "                    cooc_count = 0\n",
    "                    for doc_idx in doc_indices:\n",
    "                        if (doc_word_matrix[doc_idx, word1_idx] > 0 and \n",
    "                            doc_word_matrix[doc_idx, word2_idx] > 0):\n",
    "                            cooc_count += 1\n",
    "                    \n",
    "                    if cooc_count == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    p_w1 = freq1 / total_docs\n",
    "                    p_w2 = freq2 / total_docs\n",
    "                    p_w1_w2 = cooc_count / total_docs\n",
    "                    \n",
    "                    if p_w1 > 0 and p_w2 > 0 and p_w1_w2 > 0:\n",
    "                        pmi = np.log(p_w1_w2 / (p_w1 * p_w2))\n",
    "                        npmi = pmi / (-np.log(p_w1_w2))\n",
    "                        npmi_scores.append(npmi)\n",
    "            \n",
    "            if npmi_scores:\n",
    "                topic_npmis.append(np.mean(npmi_scores))\n",
    "\n",
    "        return np.mean(topic_npmis) if topic_npmis else 0.0\n",
    "    \n",
    "    def find_optimal_clusters(self, min_topics=2, max_topics=20, step=1):\n",
    "        \"\"\"\n",
    "        Find optimal number of clusters using NPMI with minimum cluster size constraint\n",
    "        \n",
    "        Parameters:\n",
    "        min_topics: Minimum number of topics to test\n",
    "        max_topics: Maximum number of topics to test\n",
    "        step: Step size for topic range\n",
    "        \"\"\"\n",
    "        if self.documents is None:\n",
    "            self.load_and_preprocess_data()\n",
    "        \n",
    "        results = []\n",
    "        topic_range = range(min_topics, max_topics + 1, step)\n",
    "        \n",
    "        print(f\"Testing {len(topic_range)} different cluster configurations...\")\n",
    "        print(f\"Range: {min_topics} to {max_topics} topics\")\n",
    "        print(f\"Minimum cluster size: {self.min_cluster_size}\")\n",
    "        \n",
    "        for n_topics in topic_range:\n",
    "            print(f\"\\nTesting {n_topics} topics...\")\n",
    "            \n",
    "            try:\n",
    "                from hdbscan import HDBSCAN\n",
    "                \n",
    "                hdbscan_model = HDBSCAN(\n",
    "                    min_cluster_size=self.min_cluster_size,\n",
    "                    metric='euclidean',\n",
    "                    cluster_selection_method='eom'\n",
    "                )\n",
    "                \n",
    "                topic_model = BERTopic(\n",
    "                    nr_topics=n_topics,\n",
    "                    hdbscan_model=hdbscan_model,\n",
    "                    embedding_model=self.embedding_model,\n",
    "                    calculate_probabilities=False,\n",
    "                    verbose=False\n",
    "                )\n",
    "                \n",
    "                topics, probabilities = topic_model.fit_transform(self.documents, self.embeddings)\n",
    "\n",
    "                df_temp = pd.DataFrame({'topic': topics})\n",
    "                \n",
    "                df_reassigned = self.reassign_outliers(df_temp, self.embeddings)\n",
    "                final_topics = df_reassigned['topic'].tolist()\n",
    "                \n",
    "                npmi_score = self.calculate_npmi(self.documents, final_topics)\n",
    "                \n",
    "                unique_topics = len(set(final_topics) - {-1})\n",
    "                outlier_count = sum(1 for t in final_topics if t == -1)\n",
    "                \n",
    "                topic_counts = Counter([t for t in final_topics if t != -1])\n",
    "                min_topic_size = min(topic_counts.values()) if topic_counts else 0\n",
    "                max_topic_size = max(topic_counts.values()) if topic_counts else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'n_topics_requested': n_topics,\n",
    "                    'n_topics_actual': unique_topics,\n",
    "                    'npmi_score': npmi_score,\n",
    "                    'outliers_before': sum(1 for t in topics if t == -1),\n",
    "                    'outliers_after': outlier_count,\n",
    "                    'min_topic_size': min_topic_size,\n",
    "                    'max_topic_size': max_topic_size,\n",
    "                    'topic_model': topic_model\n",
    "                })\n",
    "                \n",
    "                print(f\"  NPMI: {npmi_score:.4f}, Actual topics: {unique_topics}\")\n",
    "                print(f\"  Topic sizes - Min: {min_topic_size}, Max: {max_topic_size}\")\n",
    "                print(f\"  Outliers before/after: {results[-1]['outliers_before']}/{outlier_count}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {n_topics} topics: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if results:\n",
    "            valid_results = [r for r in results if r['n_topics_actual'] > 0]\n",
    "            \n",
    "            if valid_results:\n",
    "                best_result = max(valid_results, key=lambda x: x['npmi_score'])\n",
    "                \n",
    "                print(f\"\\n\" + \"=\"*60)\n",
    "                print(\"NPMI OPTIMIZATION RESULTS (Min Cluster Size: 30)\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"Requested topics: {best_result['n_topics_requested']}\")\n",
    "                print(f\"ACTUAL TOPICS GENERATED: {best_result['n_topics_actual']} (This is what you got!)\")\n",
    "                print(f\"Best NPMI score: {best_result['npmi_score']:.4f}\")\n",
    "                print(f\"Topic size range: {best_result['min_topic_size']} - {best_result['max_topic_size']}\")\n",
    "                print(f\"Outliers reassigned: {best_result['outliers_before']}\")\n",
    "                \n",
    "                same_actual_topics = [r for r in valid_results if r['n_topics_actual'] == best_result['n_topics_actual']]\n",
    "                if len(same_actual_topics) > 1:\n",
    "                    print(f\"\\nNote: {len(same_actual_topics)} different requests produced {best_result['n_topics_actual']} topics:\")\n",
    "                    for r in same_actual_topics[:5]:  # Show first 5\n",
    "                        print(f\"  Request {r['n_topics_requested']} → {r['n_topics_actual']} topics (NPMI: {r['npmi_score']:.4f})\")\n",
    "                    if len(same_actual_topics) > 5:\n",
    "                        print(f\"  ... and {len(same_actual_topics) - 5} more\")\n",
    "                \n",
    "                return results, best_result\n",
    "            else:\n",
    "                print(\"No valid results found!\")\n",
    "                return [], None\n",
    "        else:\n",
    "            print(\"No valid results found!\")\n",
    "            return [], None\n",
    "    \n",
    "    def plot_results(self, results):\n",
    "        \"\"\"Plot NPMI scores across different numbers of topics\"\"\"\n",
    "        if not results:\n",
    "            print(\"No results to plot!\")\n",
    "            return\n",
    "        \n",
    "        n_topics = [r['n_topics_requested'] for r in results]\n",
    "        npmi_scores = [r['npmi_score'] for r in results]\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        actual_topics = [r['n_topics_actual'] for r in results]\n",
    "        npmi_scores = [r['npmi_score'] for r in results]\n",
    "        \n",
    "        scatter = plt.scatter(actual_topics, npmi_scores, c=n_topics, cmap='viridis', s=60, alpha=0.7)\n",
    "        plt.xlabel('Actual Topics Generated')\n",
    "        plt.ylabel('NPMI Score')\n",
    "        plt.title('NPMI Score vs Actual Topics Generated\\n(Color = Requested Topics)')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter, label='Requested Topics')\n",
    "        \n",
    "        best_idx = np.argmax(npmi_scores)\n",
    "        plt.scatter(actual_topics[best_idx], npmi_scores[best_idx], \n",
    "                   color='red', s=100, marker='*', label=f'Best: {actual_topics[best_idx]} actual topics')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        actual_topics = [r['n_topics_actual'] for r in results]\n",
    "        plt.plot(n_topics, actual_topics, 'go-', linewidth=2, markersize=8, label='Actual')\n",
    "        plt.plot(n_topics, n_topics, 'r--', alpha=0.5, label='Requested')\n",
    "        plt.xlabel('Requested Topics')\n",
    "        plt.ylabel('Actual Topics')\n",
    "        plt.title('Requested vs Actual Topics')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        min_sizes = [r['min_topic_size'] for r in results]\n",
    "        max_sizes = [r['max_topic_size'] for r in results]\n",
    "        plt.plot(n_topics, min_sizes, 'b-', label='Min Topic Size', marker='o')\n",
    "        plt.plot(n_topics, max_sizes, 'r-', label='Max Topic Size', marker='s')\n",
    "        plt.axhline(y=30, color='green', linestyle='--', alpha=0.7, label='Min Cluster Size (30)')\n",
    "        plt.xlabel('Number of Topics')\n",
    "        plt.ylabel('Topic Size')\n",
    "        plt.title('Topic Size Ranges')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def get_final_model(self, best_result):\n",
    "        \"\"\"Get the final model with reassigned outliers\"\"\"\n",
    "        if best_result is None:\n",
    "            return None, None\n",
    "        \n",
    "        topic_model = best_result['topic_model']\n",
    "        \n",
    "        topics, probabilities = topic_model.fit_transform(self.documents, self.embeddings)\n",
    "        df_temp = pd.DataFrame({'topic': topics})\n",
    "        df_final = self.reassign_outliers(df_temp, self.embeddings)\n",
    "        \n",
    "        self.df['topic'] = df_final['topic']\n",
    "        \n",
    "        return topic_model, self.df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimizer = NPMIOptimizer(\n",
    "        data_path=\"\",\n",
    "        text_columns=['title'], \n",
    "        selftext_column='selftext',  \n",
    "        min_cluster_size=30  \n",
    "    )\n",
    "    \n",
    "    optimizer.load_and_preprocess_data()\n",
    "    \n",
    "\n",
    "    results, best_result = optimizer.find_optimal_clusters(\n",
    "        min_topics=1,  \n",
    "        max_topics=40,  \n",
    "        step=1    \n",
    "    )\n",
    "    \n",
    "    optimizer.plot_results(results)\n",
    "    \n",
    "    if best_result:\n",
    "        final_model, final_df = optimizer.get_final_model(best_result)\n",
    "        \n",
    "        print(\"\\nFinal topic distribution:\")\n",
    "        print(final_df['topic'].value_counts().sort_index())\n",
    "        \n",
    "        print(\"\\nTop keywords per topic:\")\n",
    "        for topic_id in sorted(final_df['topic'].unique()):\n",
    "            if topic_id != -1:\n",
    "                keywords = final_model.get_topic(topic_id)\n",
    "                top_words = [word for word, score in keywords[:5]]\n",
    "                print(f\"Topic {topic_id}: {', '.join(top_words)}\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd93763-6998-456e-bf9e-bbe8473a1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def check_and_install_openai():\n",
    "    \"\"\"Check if OpenAI package is installed\"\"\"\n",
    "    try:\n",
    "        import openai\n",
    "        from openai import OpenAI\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"Missing OpenAI package. Please install it using:\")\n",
    "        print(\"pip install openai\")\n",
    "        print(\"\\nOr with conda:\")\n",
    "        print(\"conda install -c conda-forge openai\")\n",
    "        return False\n",
    "\n",
    "if not check_and_install_openai():\n",
    "    sys.exit(\"Please install the OpenAI package and restart the kernel.\")\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "class TopicLabeler:\n",
    "    def __init__(self, api_key, final_df, documents, confidence_level=0.95):\n",
    "        \"\"\"\n",
    "        Initialize the topic labeler with ChatGPT-4o\n",
    "        \n",
    "        Parameters:\n",
    "        api_key: Your OpenAI API key\n",
    "        final_df: DataFrame with topic assignments\n",
    "        documents: List of original documents\n",
    "        confidence_level: Confidence level for sampling (default: 0.95)\n",
    "        \"\"\"\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.final_df = final_df.copy()\n",
    "        self.documents = documents\n",
    "        self.confidence_level = confidence_level\n",
    "        self.topic_labels = {}\n",
    "        \n",
    "        self.final_df['document'] = documents\n",
    "        \n",
    "    def calculate_sample_size(self, population_size, confidence_level=0.95, margin_error=0.05):\n",
    "        \"\"\"\n",
    "        Calculate required sample size for given confidence level and margin of error\n",
    "        \n",
    "        Parameters:\n",
    "        population_size: Size of the topic cluster\n",
    "        confidence_level: Confidence level (default: 0.95 for 95% CI)\n",
    "        margin_error: Margin of error (default: 0.05 for 5%)\n",
    "        \"\"\"\n",
    "        z_scores = {0.90: 1.645, 0.95: 1.96, 0.99: 2.576}\n",
    "        z = z_scores.get(confidence_level, 1.96)\n",
    "        \n",
    "        p = 0.5\n",
    "        \n",
    "        n = (z**2 * p * (1-p)) / (margin_error**2)\n",
    "        \n",
    "        if population_size <= n:\n",
    "            return population_size\n",
    "        else:\n",
    "            n_corrected = n / (1 + (n - 1) / population_size)\n",
    "            return max(int(np.ceil(n_corrected)), min(10, population_size))\n",
    "    \n",
    "    def sample_topic_posts(self, topic_id, max_sample_size=50):\n",
    "        \"\"\"\n",
    "        Sample posts from a topic using statistical sampling for 95% CI\n",
    "        \n",
    "        Parameters:\n",
    "        topic_id: The topic to sample from\n",
    "        max_sample_size: Maximum number of posts to sample (to keep API calls manageable)\n",
    "        \"\"\"\n",
    "        topic_posts = self.final_df[self.final_df['topic'] == topic_id]\n",
    "        population_size = len(topic_posts)\n",
    "        \n",
    "        if population_size == 0:\n",
    "            return []\n",
    "        \n",
    "        sample_size = self.calculate_sample_size(population_size, self.confidence_level)\n",
    "        \n",
    "        sample_size = min(sample_size, max_sample_size, population_size)\n",
    "        \n",
    "        sampled_posts = topic_posts.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"Topic {topic_id}: Population={population_size}, Sample={sample_size} ({sample_size/population_size*100:.1f}%)\")\n",
    "        \n",
    "        return sampled_posts['document'].tolist()\n",
    "    \n",
    "    def create_labeling_prompt(self, topic_id, sample_posts, top_keywords):\n",
    "        \"\"\"\n",
    "        Create a comprehensive prompt for ChatGPT-4o topic labeling\n",
    "        \n",
    "        Parameters:\n",
    "        topic_id: The topic ID\n",
    "        sample_posts: List of sampled posts for this topic\n",
    "        top_keywords: Top keywords from BERTopic for this topic\n",
    "        \"\"\"\n",
    "        keywords_str = \", \".join(top_keywords)\n",
    "        posts_str = \"\\n\\n\".join([f\"Post {i+1}: {post[:300]}{'...' if len(post) > 300 else ''}\" \n",
    "                                for i, post in enumerate(sample_posts)])\n",
    "        \n",
    "        prompt = f\"\"\"You are a helpful assistant for labeling text topics of posts on Reddit that were labelled as relevant to the topic of percieved politicization of tech companies.\n",
    "TOPIC INFORMATION:\n",
    "- Topic ID: {topic_id}\n",
    "- Top keywords from algorithm: {keywords_str}\n",
    "- Number of posts in cluster: {len(self.final_df[self.final_df['topic'] == topic_id])}\n",
    "- Sample size: {len(sample_posts)} posts (95% confidence interval)\n",
    "\n",
    "SAMPLE POSTS FROM THIS CLUSTER:\n",
    "{posts_str}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Analyze the sample posts to understand the main theme\n",
    "2. Create a clear, concise topic label (2-6 words)\n",
    "3. The label should capture the essence of what makes this cluster distinct\n",
    "5. Be specific enough to distinguish from other tech/political topics\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "Topic Label: [Your 2-6 word label]\n",
    "Brief Explanation: [1-2 sentences explaining why this label fits]\n",
    "\n",
    "Example format:\n",
    "Topic Label: Facebook Election Misinformation\n",
    "Brief Explanation: This cluster focuses on posts discussing Facebook's role in spreading misinformation during elections and the political debates around content moderation policies.\"\"\"\n",
    "\n",
    "        return prompt\n",
    "    \n",
    "    def generate_topic_labels(self, final_model, delay_between_calls=1.1, max_retries=3):\n",
    "        \"\"\"\n",
    "        Generate labels for all topics using ChatGPT-4o\n",
    "        \n",
    "        Parameters:\n",
    "        final_model: The fitted BERTopic model\n",
    "        delay_between_calls: Delay between API calls (default: 1.1 seconds)\n",
    "        max_retries: Maximum number of retries for failed calls\n",
    "        \"\"\"\n",
    "        unique_topics = sorted([t for t in self.final_df['topic'].unique() if t != -1])\n",
    "        \n",
    "        print(f\"Generating labels for {len(unique_topics)} topics using ChatGPT-4o...\")\n",
    "        print(f\"Using {self.confidence_level*100}% confidence interval sampling\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for topic_id in unique_topics:\n",
    "            print(f\"\\\\nProcessing Topic {topic_id}...\")\n",
    "            \n",
    "            try:\n",
    "                topic_keywords = final_model.get_topic(topic_id)\n",
    "                top_keywords = [word for word, score in topic_keywords[:8]]\n",
    "            except:\n",
    "                top_keywords = [f\"keyword_{i}\" for i in range(5)]\n",
    "            \n",
    "            sample_posts = self.sample_topic_posts(topic_id)\n",
    "            \n",
    "            if not sample_posts:\n",
    "                print(f\"No posts found for Topic {topic_id}\")\n",
    "                continue\n",
    "            \n",
    "            prompt = self.create_labeling_prompt(topic_id, sample_posts, top_keywords)\n",
    "            \n",
    "            success = False\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = self.client.chat.completions.create(\n",
    "                        model=\"gpt-4o\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are an expert at analyzing social media posts about technology and politics. You excel at identifying the key themes and creating concise, descriptive labels for topic clusters.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=0.3,\n",
    "                        max_tokens=200\n",
    "                    )\n",
    "                    \n",
    "                    full_response = response.choices[0].message.content.strip()\n",
    "                    \n",
    "                    if \"Topic Label:\" in full_response:\n",
    "                        label = full_response.split(\"Topic Label:\")[1].split(\"\\\\n\")[0].strip()\n",
    "                    else:\n",
    "                        label = full_response.split(\"\\\\n\")[0].strip()\n",
    "                    \n",
    "                    self.topic_labels[topic_id] = {\n",
    "                        'label': label,\n",
    "                        'full_response': full_response,\n",
    "                        'sample_size': len(sample_posts),\n",
    "                        'population_size': len(self.final_df[self.final_df['topic'] == topic_id]),\n",
    "                        'keywords': top_keywords\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"✓ Topic {topic_id}: {label}\")\n",
    "                    success = True\n",
    "                    break\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Attempt {attempt + 1} failed for Topic {topic_id}: {e}\")\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(delay_between_calls * 2)\n",
    "            \n",
    "            if not success:\n",
    "                print(f\"Failed to label Topic {topic_id} after {max_retries} attempts\")\n",
    "                self.topic_labels[topic_id] = {\n",
    "                    'label': f\"Topic {topic_id} (Failed)\",\n",
    "                    'full_response': \"Failed to generate\",\n",
    "                    'sample_size': len(sample_posts),\n",
    "                    'population_size': len(self.final_df[self.final_df['topic'] == topic_id]),\n",
    "                    'keywords': top_keywords\n",
    "                }\n",
    "            \n",
    "            if success:\n",
    "                time.sleep(delay_between_calls)\n",
    "        \n",
    "        print(\"\\\\n\" + \"=\" * 60)\n",
    "        print(\"TOPIC LABELING COMPLETED\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return self.topic_labels\n",
    "    \n",
    "    def display_results(self):\n",
    "        \"\"\"Display the generated topic labels in a nice format\"\"\"\n",
    "        if not self.topic_labels:\n",
    "            print(\"No topic labels generated yet. Run generate_topic_labels() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\\\n GENERATED TOPIC LABELS:\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        for topic_id in sorted(self.topic_labels.keys()):\n",
    "            info = self.topic_labels[topic_id]\n",
    "            print(f\"\\\\n TOPIC {topic_id}\")\n",
    "            print(f\"   Label: {info['label']}\")\n",
    "            print(f\"   Size: {info['population_size']} posts (sampled: {info['sample_size']})\")\n",
    "            print(f\"   Keywords: {', '.join(info['keywords'][:5])}\")\n",
    "            if 'Brief Explanation:' in info['full_response']:\n",
    "                explanation = info['full_response'].split('Brief Explanation:')[1].strip()\n",
    "                print(f\"   Explanation: {explanation}\")\n",
    "    \n",
    "    def export_results(self, filename=\"topic_labels_results.csv\"):\n",
    "        \"\"\"Export results to CSV\"\"\"\n",
    "        if not self.topic_labels:\n",
    "            print(\"No topic labels to export.\")\n",
    "            return\n",
    "        \n",
    "        export_data = []\n",
    "        for topic_id, info in self.topic_labels.items():\n",
    "            export_data.append({\n",
    "                'topic_id': topic_id,\n",
    "                'label': info['label'],\n",
    "                'population_size': info['population_size'],\n",
    "                'sample_size': info['sample_size'],\n",
    "                'sampling_percentage': (info['sample_size'] / info['population_size']) * 100,\n",
    "                'top_keywords': ', '.join(info['keywords'][:5]),\n",
    "                'full_response': info['full_response']\n",
    "            })\n",
    "        \n",
    "        df_export = pd.DataFrame(export_data)\n",
    "        df_export.to_csv(filename, index=False)\n",
    "        print(f\"\\\\n Results exported to: {filename}\")\n",
    "        \n",
    "        return df_export\n",
    "\n",
    "def label_your_topics(final_df, documents, final_model, api_key):\n",
    "    \"\"\"\n",
    "    Convenience function to label your specific topics\n",
    "    \"\"\"\n",
    "    labeler = TopicLabeler(\n",
    "        api_key=api_key,\n",
    "        final_df=final_df,\n",
    "        documents=documents,\n",
    "        confidence_level=0.95\n",
    "    )\n",
    "    \n",
    "    topic_labels = labeler.generate_topic_labels(final_model)\n",
    "    \n",
    "    labeler.display_results()\n",
    "    \n",
    "    results_df = labeler.export_results()\n",
    "    \n",
    "    return labeler, results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"\"\n",
    "    \n",
    "    if 'final_df' in locals() and 'optimizer' in locals() and 'final_model' in locals():\n",
    "        print(\"Starting topic labeling with ChatGPT-4o...\")\n",
    "        labeler, results_df = label_your_topics(\n",
    "            final_df=final_df,\n",
    "            documents=optimizer.documents, \n",
    "            final_model=final_model,\n",
    "            api_key=API_KEY\n",
    "        )\n",
    "        print(\"\\\\n Topic labeling completed!\")\n",
    "    else:\n",
    "        print(\"Please ensure you have 'final_df', 'optimizer', and 'final_model' variables loaded.\")\n",
    "        print(\"\\\\nTo use manually:\")\n",
    "        print(\"labeler = TopicLabeler(api_key, final_df, documents)\")\n",
    "        print(\"labels = labeler.generate_topic_labels(final_model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aaaff5-9cad-4963-877a-9c0b502f6645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_sentiment_plot(final_df, sentiment_column='llm_sentiment', figsize=(16, 10)):\n",
    "    \"\"\"\n",
    "    Create sentiment analysis plot for current topic clusters\n",
    "    \n",
    "    Parameters:\n",
    "    final_df: DataFrame with topic assignments and sentiment data\n",
    "    sentiment_column: Name of the sentiment column (default: 'llm_sentiment')\n",
    "    figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    topic_names = {\n",
    "        0: \"Elon Musk's Political Influence\",\n",
    "        1: \"Zuckerberg's Political Entanglements\", \n",
    "        2: \"Google Political Bias Allegations\",\n",
    "        3: \"Amazon's Political Influence and Controversies\",\n",
    "        4: \"Big Tech Censorship and Politics\",\n",
    "        5: \"Facebook's Political Influence and Bias\",\n",
    "        6: \"AI's Political Influence and Ethics\",\n",
    "        7: \"Microsoft Political and Social Stances\",\n",
    "        8: \"Meta's Political Controversies\",\n",
    "        9: \"Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"Social Media Bans on Trump\",\n",
    "        11: \"Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"Instagram Political Bias and Censorship\",\n",
    "        13: \"Tech Privacy and Political Influence\",\n",
    "        14: \"Parler Deplatforming Controversy\",\n",
    "        15: \"Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    if sentiment_column not in final_df.columns:\n",
    "        print(f\"Error: Column '{sentiment_column}' not found in dataframe.\")\n",
    "        print(f\"Available columns: {list(final_df.columns)}\")\n",
    "        return\n",
    "    \n",
    "    df_filtered = final_df[final_df['topic'] != -1].copy()\n",
    "\n",
    "    sentiment_counts = df_filtered.groupby(['topic', sentiment_column]).size().unstack(fill_value=0)\n",
    "    sentiment_props = sentiment_counts.div(sentiment_counts.sum(axis=1), axis=0)\n",
    "    \n",
    "    if 'Negative' in sentiment_props.columns:\n",
    "        sentiment_props_sorted = sentiment_props.sort_values(by='Negative', ascending=True)\n",
    "    elif 'negative' in sentiment_props.columns:\n",
    "        sentiment_props_sorted = sentiment_props.sort_values(by='negative', ascending=True)\n",
    "    else:\n",
    "        sentiment_props_sorted = sentiment_props.sort_index()\n",
    "        print(\"Warning: No 'Negative' sentiment column found. Sorting by topic ID instead.\")\n",
    "    \n",
    "    topic_labels = []\n",
    "    for topic_id in sentiment_props_sorted.index:\n",
    "        if topic_id in topic_names:\n",
    "            label = topic_names[topic_id]\n",
    "            if len(label) > 35:\n",
    "                label = label[:32] + \"...\"\n",
    "            topic_labels.append(f\"T{topic_id}: {label}\")\n",
    "        else:\n",
    "            topic_labels.append(f\"Topic {topic_id}\")\n",
    "    \n",
    "    sentiment_props_sorted.index = topic_labels\n",
    "    \n",
    "    available_sentiments = sentiment_props_sorted.columns.tolist()\n",
    "    \n",
    "    sentiment_mapping = {}\n",
    "    colors_mapping = {}\n",
    "    \n",
    "    for col in available_sentiments:\n",
    "        col_lower = col.lower()\n",
    "        if 'pos' in col_lower:\n",
    "            sentiment_mapping['Positive'] = col\n",
    "            colors_mapping['Positive'] = '#8EAC50'\n",
    "        elif 'neu' in col_lower:\n",
    "            sentiment_mapping['Neutral'] = col\n",
    "            colors_mapping['Neutral'] = '#FFE17B'\n",
    "        elif 'neg' in col_lower:\n",
    "            sentiment_mapping['Negative'] = col\n",
    "            colors_mapping['Negative'] = '#FD8D14'\n",
    "    \n",
    "    if not sentiment_mapping:\n",
    "        sentiment_order = available_sentiments\n",
    "        colors = ['#A0C878', '#DDEB9D', '#EB5B00'][:len(sentiment_order)]\n",
    "    else:\n",
    "        sentiment_order = [sentiment_mapping.get(s, s) for s in ['Positive', 'Neutral', 'Negative'] if sentiment_mapping.get(s) in available_sentiments]\n",
    "        colors = [colors_mapping.get(s, '#DDEB9D') for s in ['Positive', 'Neutral', 'Negative'] if sentiment_mapping.get(s) in available_sentiments]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    sentiment_props_sorted[sentiment_order].plot(\n",
    "        kind='barh',\n",
    "        stacked=True,\n",
    "        color=colors,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"Sentiment Proportion per Topic Cluster\", fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel(\"Proportion of Posts\", fontsize=12)\n",
    "    ax.set_ylabel(\"Topic Cluster\", fontsize=12)\n",
    "    \n",
    "    legend_labels = [s.title() for s in sentiment_order]\n",
    "    ax.legend(legend_labels, title=\"Sentiment\", bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    \n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(\"SENTIMENT ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total posts analyzed: {len(df_filtered):,}\")\n",
    "    print(f\"Topics analyzed: {len(sentiment_props_sorted)}\")\n",
    "    print(f\"Sentiment categories: {sentiment_order}\")\n",
    "    print(\"\\nMost negative topics (highest negative sentiment):\")\n",
    "    \n",
    "    if 'Negative' in sentiment_mapping and sentiment_mapping['Negative'] in sentiment_props_sorted.columns:\n",
    "        negative_col = sentiment_mapping['Negative']\n",
    "        most_negative = sentiment_props_sorted.sort_values(by=negative_col, ascending=False).head(3)\n",
    "        for i, (topic, row) in enumerate(most_negative.iterrows(), 1):\n",
    "            print(f\"  {i}. {topic}: {row[negative_col]:.1%} negative\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return sentiment_props_sorted\n",
    "\n",
    "def print_sentiment_statistics(final_df, sentiment_column='llm_sentiment'):\n",
    "    \"\"\"\n",
    "    Print detailed sentiment statistics for each topic\n",
    "    \"\"\"\n",
    "    topic_names = {\n",
    "        0: \"Elon Musk's Political Influence\",\n",
    "        1: \"Zuckerberg's Political Entanglements\", \n",
    "        2: \"Google Political Bias Allegations\",\n",
    "        3: \"Amazon's Political Influence and Controversies\",\n",
    "        4: \"Big Tech Censorship and Politics\",\n",
    "        5: \"Facebook's Political Influence and Bias\",\n",
    "        6: \"AI's Political Influence and Ethics\",\n",
    "        7: \"Microsoft Political and Social Stances\",\n",
    "        8: \"Meta's Political Controversies\",\n",
    "        9: \"Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"Social Media Bans on Trump\",\n",
    "        11: \"Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"Instagram Political Bias and Censorship\",\n",
    "        13: \"Tech Privacy and Political Influence\",\n",
    "        14: \"Parler Deplatforming Controversy\",\n",
    "        15: \"Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    if sentiment_column not in final_df.columns:\n",
    "        print(f\"Error: Column '{sentiment_column}' not found.\")\n",
    "        return\n",
    "    \n",
    "    df_filtered = final_df[final_df['topic'] != -1].copy()\n",
    "    \n",
    "    print(\"\\nDETAILED SENTIMENT STATISTICS BY TOPIC\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for topic_id in sorted(df_filtered['topic'].unique()):\n",
    "        topic_data = df_filtered[df_filtered['topic'] == topic_id]\n",
    "        topic_name = topic_names.get(topic_id, f\"Topic {topic_id}\")\n",
    "        \n",
    "        print(f\"\\n  TOPIC {topic_id}: {topic_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Total posts: {len(topic_data):,}\")\n",
    "        \n",
    "        sentiment_counts = topic_data[sentiment_column].value_counts()\n",
    "        sentiment_props = topic_data[sentiment_column].value_counts(normalize=True)\n",
    "        \n",
    "        for sentiment in sentiment_counts.index:\n",
    "            count = sentiment_counts[sentiment]\n",
    "            prop = sentiment_props[sentiment]\n",
    "            print(f\"  {sentiment}: {count:,} posts ({prop:.1%})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'final_df' in locals():\n",
    "        print(\"Available columns in final_df:\")\n",
    "        print(final_df.columns.tolist())\n",
    "        print()\n",
    "        \n",
    "        sentiment_cols = [col for col in final_df.columns if 'sentiment' in col.lower()]\n",
    "        \n",
    "        if sentiment_cols:\n",
    "            print(f\"Found sentiment columns: {sentiment_cols}\")\n",
    "            sentiment_col = sentiment_cols[0]\n",
    "            print(f\"Using column: {sentiment_col}\")\n",
    "            \n",
    "            sentiment_props = create_sentiment_plot(final_df, sentiment_column=sentiment_col)\n",
    "            \n",
    "            print_sentiment_statistics(final_df, sentiment_column=sentiment_col)\n",
    "            \n",
    "        else:\n",
    "            print(\"No sentiment columns found. Please ensure your dataframe has sentiment data.\")\n",
    "            print(\"Expected column names: 'llm_sentiment', 'sentiment', etc.\")\n",
    "    else:\n",
    "        print(\"Please ensure 'final_df' variable is loaded.\")\n",
    "        print(\"\\nTo use this script:\")\n",
    "        print(\"sentiment_props = create_sentiment_plot(final_df, sentiment_column='your_sentiment_column')\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40051d4b-dfa1-47a5-a83c-e45a116a4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_weekly_topic_trends(final_df, date_column='created_utc', figsize=(14, 8)):\n",
    "    \"\"\"\n",
    "    Create weekly topic trends plot for current topic clusters\n",
    "    \n",
    "    Parameters:\n",
    "    final_df: DataFrame with topic assignments and date data\n",
    "    date_column: Name of the date column (default: 'created_utc')\n",
    "    figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "\n",
    "    topic_labels = {\n",
    "        0: \"Elon Musk's Political Influence\",\n",
    "        1: \"Zuckerberg's Political Entanglements\", \n",
    "        2: \"Google Political Bias Allegations\",\n",
    "        3: \"Amazon's Political Influence and Controversies\",\n",
    "        4: \"Big Tech Censorship and Politics\",\n",
    "        5: \"Facebook's Political Influence and Bias\",\n",
    "        6: \"AI's Political Influence and Ethics\",\n",
    "        7: \"Microsoft Political and Social Stances\",\n",
    "        8: \"Meta's Political Controversies\",\n",
    "        9: \"Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"Social Media Bans on Trump\",\n",
    "        11: \"Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"Instagram Political Bias and Censorship\",\n",
    "        13: \"Tech Privacy and Political Influence\",\n",
    "        14: \"Parler Deplatforming Controversy\",\n",
    "        15: \"Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    if date_column not in final_df.columns:\n",
    "        print(f\"Error: Column '{date_column}' not found in dataframe.\")\n",
    "        print(f\"Available columns: {list(final_df.columns)}\")\n",
    "        return\n",
    "    \n",
    "    df = final_df.copy()\n",
    "    \n",
    "    try:\n",
    "        if df[date_column].dtype == 'object':\n",
    "            df[date_column] = pd.to_datetime(df[date_column])\n",
    "        elif df[date_column].dtype in ['int64', 'float64']:\n",
    "            df[date_column] = pd.to_datetime(df[date_column], unit='s')\n",
    "        else:\n",
    "            df[date_column] = pd.to_datetime(df[date_column])\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting date column: {e}\")\n",
    "        return\n",
    "    \n",
    "    df_filtered = df[df[date_column].dt.year.between(2020, 2025)]\n",
    "    \n",
    "    df_filtered = df_filtered[df_filtered['topic'] != -1]\n",
    "    \n",
    "    print(f\"Data range: {df_filtered[date_column].min()} to {df_filtered[date_column].max()}\")\n",
    "    print(f\"Total posts: {len(df_filtered):,}\")\n",
    "    \n",
    "    weekly_counts = df_filtered.groupby([pd.Grouper(key=date_column, freq='W'), 'topic']).size().unstack(fill_value=0)\n",
    "    \n",
    "    weekly_counts = weekly_counts.rename(columns=topic_labels)\n",
    "    \n",
    "    top_5 = weekly_counts.sum().sort_values(ascending=False).head(5).index.tolist()\n",
    "    \n",
    "    print(f\"\\\\nTop 5 topics by volume:\")\n",
    "    for i, topic in enumerate(top_5, 1):\n",
    "        total = weekly_counts[topic].sum()\n",
    "        print(f\"  {i}. {topic}: {total:,} posts\")\n",
    "    \n",
    "    weekly_counts['Others'] = weekly_counts.drop(columns=top_5).sum(axis=1)\n",
    "    weekly_top5 = weekly_counts[top_5 + ['Others']]\n",
    "    \n",
    "    weekly_top5_smoothed = weekly_top5.rolling(window=4, min_periods=1).mean()\n",
    "    \n",
    "    colors = {\n",
    "        top_5[0]: '#A2C579',  # Light green\n",
    "        top_5[1]: '#98ABEE',  # Light blue\n",
    "        top_5[2]: '#1D24CA',  # Blue\n",
    "        top_5[3]: '#201658',  # Dark blue\n",
    "        top_5[4]: '#6554c0',  # Purple\n",
    "        'Others': '#999999'   # Gray\n",
    "    }\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=figsize, \n",
    "                                   gridspec_kw={'height_ratios': [1, 2]})\n",
    "    \n",
    "    weekly_top5_smoothed.plot.area(ax=ax1, stacked=True, legend=False, linewidth=0, \n",
    "                                   alpha=0.95, color=[colors[col] for col in weekly_top5_smoothed.columns])\n",
    "    weekly_top5_smoothed.plot.area(ax=ax2, stacked=True, legend=True, linewidth=0, \n",
    "                                   alpha=0.95, color=[colors[col] for col in weekly_top5_smoothed.columns])\n",
    "    \n",
    "    max_val = weekly_top5_smoothed.sum(axis=1).max()\n",
    "    ax1.set_ylim(max_val * 0.3, max_val + max_val * 0.1)\n",
    "    ax2.set_ylim(0, max_val * 0.27)\n",
    "    \n",
    "    ax1.spines['bottom'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax1.tick_params(labeltop=False)\n",
    "    ax2.xaxis.tick_bottom()\n",
    "\n",
    "    d = .015\n",
    "    kwargs = dict(transform=ax1.transAxes, color='k', clip_on=False)\n",
    "    ax1.plot((-d, +d), (-d, +d), **kwargs)\n",
    "    ax1.plot((1 - d, 1 + d), (-d, +d), **kwargs)\n",
    "    \n",
    "    kwargs.update(transform=ax2.transAxes)\n",
    "    ax2.plot((-d, +d), (1 - d, 1 + d), **kwargs)\n",
    "    ax2.plot((1 - d, 1 + d), (1 - d, 1 + d), **kwargs)\n",
    "    \n",
    "    ax2.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Post Count\", fontsize=12)\n",
    "    fig.suptitle(\"Weekly Post Volume by Topic (2020–2025)\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    ax2.legend(title=\"Topic\", bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(hspace=0.05)\n",
    "    \n",
    "    total_weeks = len(weekly_top5_smoothed)\n",
    "    avg_weekly = weekly_top5_smoothed.sum(axis=1).mean()\n",
    "    \n",
    "    print(f\"\\\\nSummary:\")\n",
    "    print(f\"  Time period: {total_weeks} weeks\")\n",
    "    print(f\"  Average weekly posts: {avg_weekly:.1f}\")\n",
    "    print(f\"  Peak week: {weekly_top5_smoothed.sum(axis=1).max():.0f} posts\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return weekly_top5_smoothed\n",
    "\n",
    "def create_simple_topic_trends(final_df, date_column='created_utc', top_n=5):\n",
    "    \"\"\"\n",
    "    Create a simpler version without broken axis\n",
    "    \"\"\"\n",
    "    topic_labels = {\n",
    "        0: \"Elon Musk's Political Influence\",\n",
    "        1: \"Zuckerberg's Political Entanglements\", \n",
    "        2: \"Google Political Bias Allegations\",\n",
    "        3: \"Amazon's Political Influence and Controversies\",\n",
    "        4: \"Big Tech Censorship and Politics\",\n",
    "        5: \"Facebook's Political Influence and Bias\",\n",
    "        6: \"AI's Political Influence and Ethics\",\n",
    "        7: \"Microsoft Political and Social Stances\",\n",
    "        8: \"Meta's Political Controversies\",\n",
    "        9: \"Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"Social Media Bans on Trump\",\n",
    "        11: \"Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"Instagram Political Bias and Censorship\",\n",
    "        13: \"Tech Privacy and Political Influence\",\n",
    "        14: \"Parler Deplatforming Controversy\",\n",
    "        15: \"Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    df = final_df.copy()\n",
    "    \n",
    "    if df[date_column].dtype in ['int64', 'float64']:\n",
    "        df[date_column] = pd.to_datetime(df[date_column], unit='s')\n",
    "    else:\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "    \n",
    "    df_filtered = df[df[date_column].dt.year.between(2020, 2025)]\n",
    "    df_filtered = df_filtered[df_filtered['topic'] != -1]\n",
    "    \n",
    "    weekly_counts = df_filtered.groupby([pd.Grouper(key=date_column, freq='W'), 'topic']).size().unstack(fill_value=0)\n",
    "    weekly_counts = weekly_counts.rename(columns=topic_labels)\n",
    "    \n",
    "    top_topics = weekly_counts.sum().sort_values(ascending=False).head(top_n).index.tolist()\n",
    "    weekly_top = weekly_counts[top_topics]\n",
    "    \n",
    "    weekly_smoothed = weekly_top.rolling(window=4, min_periods=1).mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    colors = ['#A2C579', '#98ABEE', '#1D24CA', '#201658', '#6554c0']\n",
    "    \n",
    "    for i, topic in enumerate(weekly_smoothed.columns):\n",
    "        ax.plot(weekly_smoothed.index, weekly_smoothed[topic], \n",
    "               label=topic, linewidth=2.5, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax.set_ylabel(\"Weekly Post Count\", fontsize=12)\n",
    "    ax.set_title(f\"Top {top_n} Topics: Weekly Post Trends (2020-2025)\", fontsize=14, fontweight='bold')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return weekly_smoothed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'final_df' in locals():\n",
    "        date_cols = [col for col in final_df.columns if 'date' in col.lower() or 'time' in col.lower() or 'created' in col.lower()]\n",
    "        \n",
    "        if date_cols:\n",
    "            print(f\"Found date columns: {date_cols}\")\n",
    "            date_col = date_cols[0]\n",
    "            print(f\"Using column: {date_col}\")\n",
    "            \n",
    "            weekly_data = create_weekly_topic_trends(final_df, date_column=date_col)\n",
    "            \n",
    "        else:\n",
    "            print(\"No date columns found. Please ensure your dataframe has date/time data.\")\n",
    "            print(\"Expected column names: 'created_utc', 'date', 'timestamp', etc.\")\n",
    "    else:\n",
    "        print(\"Please ensure 'final_df' variable is loaded.\")\n",
    "        print(\"\\\\nTo use this script:\")\n",
    "        print(\"weekly_data = create_weekly_topic_trends(final_df, date_column='your_date_column')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0408ad8-ba7c-44f4-b343-ea8e8f157fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extract_representative_posts(final_df, embeddings, method='centroid_closest', n_posts=1):\n",
    "    \"\"\"\n",
    "    Extract representative posts for each topic cluster\n",
    "    \n",
    "    Parameters:\n",
    "    final_df: DataFrame with topic assignments\n",
    "    embeddings: Document embeddings\n",
    "    method: 'centroid_closest', 'highest_similarity', or 'random'\n",
    "    n_posts: Number of representative posts per topic\n",
    "    \n",
    "    Returns:\n",
    "    Dictionary with topic_id as key and representative posts as values\n",
    "    \"\"\"\n",
    "    \n",
    "    topic_names = {\n",
    "        0: \"Elon Musk's Political Influence\",\n",
    "        1: \"Zuckerberg's Political Entanglements\", \n",
    "        2: \"Google Political Bias Allegations\",\n",
    "        3: \"Amazon's Political Influence and Controversies\",\n",
    "        4: \"Big Tech Censorship and Politics\",\n",
    "        5: \"Facebook's Political Influence and Bias\",\n",
    "        6: \"AI's Political Influence and Ethics\",\n",
    "        7: \"Microsoft Political and Social Stances\",\n",
    "        8: \"Meta's Political Controversies\",\n",
    "        9: \"Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"Social Media Bans on Trump\",\n",
    "        11: \"Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"Instagram Political Bias and Censorship\",\n",
    "        13: \"Tech Privacy and Political Influence\",\n",
    "        14: \"Parler Deplatforming Controversy\",\n",
    "        15: \"Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    df_with_docs = final_df.copy()\n",
    "    if 'document' not in df_with_docs.columns:\n",
    "        if 'optimizer' in globals():\n",
    "            df_with_docs['document'] = optimizer.documents\n",
    "        else:\n",
    "            print(\"Warning: No document text found. Using index as placeholder.\")\n",
    "            df_with_docs['document'] = [f\"Document {i}\" for i in range(len(df_with_docs))]\n",
    "    \n",
    "    representative_posts = {}\n",
    "    unique_topics = sorted([t for t in final_df['topic'].unique() if t != -1])\n",
    "    \n",
    "    print(f\"Extracting {n_posts} representative post(s) per topic using '{method}' method...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for topic_id in unique_topics:\n",
    "        topic_mask = final_df['topic'] == topic_id\n",
    "        topic_indices = final_df[topic_mask].index\n",
    "        topic_docs = df_with_docs[topic_mask]\n",
    "        topic_embeddings = embeddings[topic_indices]\n",
    "        \n",
    "        if len(topic_docs) == 0:\n",
    "            continue\n",
    "        \n",
    "        if method == 'centroid_closest':\n",
    "            # Find documents closest to topic centroid\n",
    "            centroid = np.mean(topic_embeddings, axis=0)\n",
    "            similarities = cosine_similarity(topic_embeddings, centroid.reshape(1, -1)).flatten()\n",
    "            top_indices = similarities.argsort()[-n_posts:][::-1]\n",
    "            \n",
    "        elif method == 'highest_similarity':\n",
    "            avg_similarities = []\n",
    "            for i, emb in enumerate(topic_embeddings):\n",
    "                other_embeddings = np.delete(topic_embeddings, i, axis=0)\n",
    "                if len(other_embeddings) > 0:\n",
    "                    avg_sim = cosine_similarity(emb.reshape(1, -1), other_embeddings).mean()\n",
    "                else:\n",
    "                    avg_sim = 0\n",
    "                avg_similarities.append(avg_sim)\n",
    "            top_indices = np.argsort(avg_similarities)[-n_posts:][::-1]\n",
    "            \n",
    "        elif method == 'random':\n",
    "            top_indices = np.random.choice(len(topic_docs), min(n_posts, len(topic_docs)), replace=False)\n",
    "        \n",
    "        rep_posts = []\n",
    "        for idx in top_indices:\n",
    "            doc_info = {\n",
    "                'document_index': topic_indices[idx],\n",
    "                'text': topic_docs.iloc[idx]['document'],\n",
    "                'similarity_score': similarities[idx] if method == 'centroid_closest' else None\n",
    "            }\n",
    "            rep_posts.append(doc_info)\n",
    "        \n",
    "        representative_posts[topic_id] = {\n",
    "            'topic_name': topic_names.get(topic_id, f\"Topic {topic_id}\"),\n",
    "            'cluster_size': len(topic_docs),\n",
    "            'representative_posts': rep_posts\n",
    "        }\n",
    "        \n",
    "        topic_name = topic_names.get(topic_id, f\"Topic {topic_id}\")\n",
    "        print(f\"\\n  TOPIC {topic_id}: {topic_name}\")\n",
    "        print(f\"Cluster size: {len(topic_docs)} posts\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for i, post in enumerate(rep_posts, 1):\n",
    "            text = post['text']\n",
    "            if len(text) > 200:\n",
    "                text = text[:200] + \"...\"\n",
    "            \n",
    "            print(f\"Representative Post {i}:\")\n",
    "            if post['similarity_score'] is not None:\n",
    "                print(f\"  Similarity to centroid: {post['similarity_score']:.4f}\")\n",
    "            print(f\"  Text: {text}\")\n",
    "            print()\n",
    "    \n",
    "    return representative_posts\n",
    "\n",
    "def export_representative_posts(representative_posts, filename=\"representative_posts.csv\"):\n",
    "    \"\"\"\n",
    "    Export representative posts to CSV file\n",
    "    \"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for topic_id, info in representative_posts.items():\n",
    "        topic_name = info['topic_name']\n",
    "        cluster_size = info['cluster_size']\n",
    "        \n",
    "        for i, post in enumerate(info['representative_posts'], 1):\n",
    "            export_data.append({\n",
    "                'topic_id': topic_id,\n",
    "                'topic_name': topic_name,\n",
    "                'cluster_size': cluster_size,\n",
    "                'representative_post_rank': i,\n",
    "                'document_index': post['document_index'],\n",
    "                'similarity_score': post['similarity_score'],\n",
    "                'post_text': post['text']\n",
    "            })\n",
    "    \n",
    "    df_export = pd.DataFrame(export_data)\n",
    "    df_export.to_csv(filename, index=False)\n",
    "    print(f\"\\n Representative posts exported to: {filename}\")\n",
    "    \n",
    "    return df_export\n",
    "\n",
    "def compare_extraction_methods(final_df, embeddings, n_posts=1):\n",
    "    \"\"\"\n",
    "    Compare different methods for extracting representative posts\n",
    "    \"\"\"\n",
    "    methods = ['centroid_closest', 'highest_similarity', 'random']\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Comparing extraction methods...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\n Method: {method.upper()}\")\n",
    "        results[method] = extract_representative_posts(final_df, embeddings, method, n_posts)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_representative_posts_for_topic(final_df, embeddings, topic_id, n_posts=3):\n",
    "    \"\"\"\n",
    "    Get multiple representative posts for a specific topic\n",
    "    \"\"\"\n",
    "    topic_names = {\n",
    "        0: \"Elon Musk's Political Influence\",\n",
    "        1: \"Zuckerberg's Political Entanglements\", \n",
    "        2: \"Google Political Bias Allegations\",\n",
    "        3: \"Amazon's Political Influence and Controversies\",\n",
    "        4: \"Big Tech Censorship and Politics\",\n",
    "        5: \"Facebook's Political Influence and Bias\",\n",
    "        6: \"AI's Political Influence and Ethics\",\n",
    "        7: \"Microsoft Political and Social Stances\",\n",
    "        8: \"Meta's Political Controversies\",\n",
    "        9: \"Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"Social Media Bans on Trump\",\n",
    "        11: \"Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"Instagram Political Bias and Censorship\",\n",
    "        13: \"Tech Privacy and Political Influence\",\n",
    "        14: \"Parler Deplatforming Controversy\",\n",
    "        15: \"Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    topic_mask = final_df['topic'] == topic_id\n",
    "    topic_indices = final_df[topic_mask].index\n",
    "    topic_embeddings = embeddings[topic_indices]\n",
    "    \n",
    "    if len(topic_indices) == 0:\n",
    "        print(f\"No documents found for topic {topic_id}\")\n",
    "        return\n",
    "    \n",
    "    if 'optimizer' in globals():\n",
    "        documents = [optimizer.documents[i] for i in topic_indices]\n",
    "    else:\n",
    "        documents = [f\"Document {i}\" for i in topic_indices]\n",
    "    \n",
    "    centroid = np.mean(topic_embeddings, axis=0)\n",
    "    similarities = cosine_similarity(topic_embeddings, centroid.reshape(1, -1)).flatten()\n",
    "    top_indices = similarities.argsort()[-n_posts:][::-1]\n",
    "    \n",
    "    topic_name = topic_names.get(topic_id, f\"Topic {topic_id}\")\n",
    "    print(f\"\\n  TOPIC {topic_id}: {topic_name}\")\n",
    "    print(f\"Showing top {n_posts} representative posts:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        print(f\"\\n{i}. Similarity: {similarities[idx]:.4f}\")\n",
    "        print(f\"   Document Index: {topic_indices[idx]}\")\n",
    "        print(f\"   Text: {documents[idx]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'final_df' in locals() and 'optimizer' in locals():\n",
    "        print(\"Extracting representative posts for all topics...\")\n",
    "        \n",
    "        rep_posts = extract_representative_posts(final_df, optimizer.embeddings, method='centroid_closest', n_posts=3)\n",
    "        \n",
    "        export_df = export_representative_posts(rep_posts)\n",
    "        \n",
    "        print(f\"\\n Extracted representative posts for {len(rep_posts)} topics\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Please ensure 'final_df' and 'optimizer' variables are loaded.\")\n",
    "        print(\"\\nAvailable functions:\")\n",
    "        print(\"• extract_representative_posts(final_df, embeddings)\")\n",
    "        print(\"• get_representative_posts_for_topic(final_df, embeddings, topic_id)\")\n",
    "        print(\"• compare_extraction_methods(final_df, embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e8edf-ba42-4dda-8ae0-cb14e9ced708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def create_simple_topic_plot(embeddings, topic_assignments, figsize=(12, 8), save_path=None):\n",
    "    \"\"\"\n",
    "    Create a simple static plot with small points and no labels on graph\n",
    "    \"\"\"\n",
    "    topic_names = {\n",
    "        0: \"T0: Elon Musk's Political Influence\",\n",
    "        1: \"T1: Zuckerberg's Political Entanglements\", \n",
    "        2: \"T2: Google Political Bias Allegations\",\n",
    "        3: \"T3: Amazon's Political Influence and Controversies\",\n",
    "        4: \"T4: Big Tech Censorship and Politics\",\n",
    "        5: \"T5: Facebook's Political Influence and Bias\",\n",
    "        6: \"T6: AI's Political Influence and Ethics\",\n",
    "        7: \"T7: Microsoft Political and Social Stances\",\n",
    "        8: \"T8: Meta's Political Controversies\",\n",
    "        9: \"T9: Tech Companies and Israel-Palestine Conflict\",\n",
    "        10: \"T10: Social Media Bans on Trump\",\n",
    "        11: \"T11: Tech Companies and Vaccine Misinformation\",\n",
    "        12: \"T12: Instagram Political Bias and Censorship\",\n",
    "        13: \"T13: Tech Privacy and Political Influence\",\n",
    "        14: \"T14: Parler Deplatforming Controversy\",\n",
    "        15: \"T15: Tech Companies and LGBTQ+ Issues\"\n",
    "    }\n",
    "    \n",
    "    topic_sizes = {\n",
    "        0: 1003, 1: 605, 2: 441, 3: 342, 4: 401, 5: 564,\n",
    "        6: 219, 7: 134, 8: 155, 9: 106, 10: 171, 11: 88,\n",
    "        12: 124, 13: 71, 14: 38, 15: 65\n",
    "    }\n",
    "\n",
    "    print(\"Computing t-SNE...\")\n",
    "    pca_50 = PCA(n_components=50, random_state=42)\n",
    "    embeddings_50d = pca_50.fit_transform(embeddings)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "    coords = tsne.fit_transform(embeddings_50d)\n",
    "    print(\"t-SNE completed!\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    unique_topics = sorted(set(topic_assignments))\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(unique_topics)))\n",
    "    \n",
    "    for i, topic in enumerate(unique_topics):\n",
    "        mask = np.array(topic_assignments) == topic\n",
    "        topic_coords = coords[mask]\n",
    "        \n",
    "        if len(topic_coords) > 0:\n",
    "            topic_name = topic_names.get(topic, f\"Topic {topic}\")\n",
    "            topic_size = topic_sizes.get(topic, len(topic_coords))\n",
    "            \n",
    "            ax.scatter(topic_coords[:, 0], topic_coords[:, 1], \n",
    "                      c=[colors[i]], alpha=0.7, s=8,  # Small point size (8)\n",
    "                      label=f\"{topic_name} (n={topic_size})\")\n",
    "            \n",
    "            centroid = np.mean(topic_coords, axis=0)\n",
    "            ax.scatter(centroid[0], centroid[1], \n",
    "                      c=[colors[i]], s=200, marker='*', \n",
    "                      edgecolors='black', linewidth=1.5)\n",
    "            \n",
    "\n",
    "            display_name = topic_name\n",
    "            if len(display_name) > 25:\n",
    "                display_name = display_name[:22] + \"...\"\n",
    "            \n",
    "            ax.annotate(f\"{display_name}\", \n",
    "                       centroid, xytext=(5, 5), \n",
    "                       textcoords='offset points', fontsize=8,\n",
    "                       bbox=dict(boxstyle='round,pad=0.3', \n",
    "                               facecolor=colors[i], alpha=0.9),\n",
    "                       ha='left')\n",
    "    \n",
    "    ax.set_xlabel('t-SNE Component 1', fontsize=12)\n",
    "    ax.set_ylabel('t-SNE Component 2', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('topic_scatter_plot.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if 'optimizer' in locals() and 'final_df' in locals():\n",
    "    create_simple_topic_plot(\n",
    "        embeddings=optimizer.embeddings,\n",
    "        topic_assignments=final_df['topic'].tolist(),\n",
    "        figsize=(15, 10)\n",
    "    )\n",
    "else:\n",
    "    print(\"Please ensure 'optimizer' and 'final_df' variables are loaded.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
